name: Auto Generate and Run Tests

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]

jobs:
  generate-and-run-tests:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest
        pip install azure-openai
        pip install langchain
        pip install pytest-cov
        # Add any other dependencies your project needs
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        
    - name: Generate tests with Azure OpenAI
      env:
        AZURE_OPENAI_API_KEY: ${{ secrets.AZURE_OPENAI_API_KEY }}
        AZURE_OPENAI_ENDPOINT: ${{ secrets.AZURE_OPENAI_ENDPOINT }}
      run: |
        # Create the test generator script
        cat > test_generator.py << 'EOF'
import os
import glob
import re
from langchain.chat_models import AzureChatOpenAI
from langchain.schema import HumanMessage
import json

# Initialize Azure OpenAI
llm = AzureChatOpenAI(
    azure_deployment="gpt4o",
    api_version="2024-08-01-preview",
    api_key=os.environ.get("AZURE_OPENAI_API_KEY"),
    azure_endpoint=os.environ.get("AZURE_OPENAI_ENDPOINT"),
    temperature=0
)

def get_all_code_files(folder_path='.', extensions=('.py',), exclude_patterns=('test_*.py', '*_test.py', 'venv/*', '.*/*')):
    all_files = []
    
    for ext in extensions:
        pattern = f"{folder_path}/**/*{ext}"
        files = glob.glob(pattern, recursive=True)
        
        # Filter out excluded patterns
        for file_path in files:
            should_exclude = False
            for exclude in exclude_patterns:
                if re.match(f".*{exclude.replace('*', '.*')}$", file_path):
                    should_exclude = True
                    break
            
            if not should_exclude:
                all_files.append(file_path)
                
    return all_files

def generate_test_for_file(file_path):
    with open(file_path, 'r') as f:
        file_content = f.read()
    
    # Extract filename without extension for the test name
    file_name = os.path.basename(file_path)
    base_name, ext = os.path.splitext(file_name)
    test_file_name = f"test_{base_name}{ext}"
    
    # Path for the test file, putting it in the same directory as the original file
    test_file_path = os.path.join(os.path.dirname(file_path), test_file_name)
    
    prompt = f"""
    You are an expert test writer. I need you to generate comprehensive pytest test cases for this code:
    
    ```
    {file_content}
    ```
    
    Please generate thorough test cases that:
    1. Cover all functions and methods
    2. Test edge cases and error conditions
    3. Achieve high code coverage
    4. Use pytest fixtures when appropriate
    5. Include docstrings explaining test purposes
    
    Return only valid Python code for the test file without any explanations or formatting.
    """
    
    # Generate the test content using Azure OpenAI
    messages = [HumanMessage(content=prompt)]
    response = llm.invoke(messages)
    test_content = response.content
    
    # Clean up the response to extract just the code
    if "```python" in test_content:
        test_content = test_content.split("```python")[1].split("```")[0].strip()
    elif "```" in test_content:
        test_content = test_content.split("```")[1].split("```")[0].strip()
    
    # Write the test file
    with open(test_file_path, 'w') as f:
        f.write(test_content)
    
    return test_file_path

def main():
    # Identify all Python files in the project
    code_files = get_all_code_files(extensions=('.py',))
    
    print(f"Found {len(code_files)} Python files to generate tests for")
    
    # Generate tests for each file
    generated_tests = []
    for file_path in code_files:
        print(f"Generating tests for {file_path}")
        try:
            test_path = generate_test_for_file(file_path)
            generated_tests.append(test_path)
            print(f"✅ Generated test file: {test_path}")
        except Exception as e:
            print(f"❌ Failed to generate test for {file_path}: {str(e)}")
    
    print(f"Successfully generated {len(generated_tests)} test files")
    
    # Write a summary of generated tests
    summary = {
        "total_files_analyzed": len(code_files),
        "tests_generated": len(generated_tests),
        "test_files": generated_tests
    }
    
    with open('test_generation_summary.json', 'w') as f:
        json.dump(summary, f, indent=2)
    
    print("Test generation complete!")

if __name__ == "__main__":
    main()
EOF
        
        # Run the test generator
        python test_generator.py
        
    - name: Run generated tests
      run: |
        python -m pytest -v --cov=./ --cov-report=xml --cov-report=term

    - name: Create test summary report
      if: always()  # Run even if tests fail
      run: |
        echo "# Test Results Summary" > test_report.md
        echo "" >> test_report.md
        echo "## Generated Test Files" >> test_report.md
        echo "" >> test_report.md
        if [ -f test_generation_summary.json ]; then
          echo "The following test files were generated:" >> test_report.md
          echo "" >> test_report.md
          cat test_generation_summary.json | python -c "import json, sys; data = json.load(sys.stdin); print('\n'.join(['- ' + file for file in data['test_files']]))" >> test_report.md
          echo "" >> test_report.md
          echo "Total files analyzed: $(cat test_generation_summary.json | python -c 'import json, sys; print(json.load(sys.stdin)["total_files_analyzed"])')" >> test_report.md
          echo "Tests generated: $(cat test_generation_summary.json | python -c 'import json, sys; print(json.load(sys.stdin)["tests_generated"])')" >> test_report.md
        else
          echo "No test generation summary available." >> test_report.md
        fi
        
        echo "" >> test_report.md
        echo "## Test Execution Results" >> test_report.md
        echo "" >> test_report.md
        if [ -f coverage.xml ]; then
          COVERAGE=$(python -c "import xml.etree.ElementTree as ET; tree = ET.parse('coverage.xml'); root = tree.getroot(); print(root.attrib['line-rate'])")
          COVERAGE_PCT=$(python -c "print(round(float('${COVERAGE}') * 100, 2))")
          echo "Overall code coverage: ${COVERAGE_PCT}%" >> test_report.md
        else
          echo "No coverage information available." >> test_report.md
        fi
        
        cat test_report.md
        
    - name: Upload test report
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: test-report
        path: |
          test_report.md
          test_generation_summary.json
          coverage.xml
